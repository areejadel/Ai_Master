{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extending KANICE for Plant and Object Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ghaida Aluhaiby ,Areej Bawazir ,Reem Qaid, 2024* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project investigates the scalability and generalizability of the Kolmogorovâ€“Arnold Networks with Interactive Convolutional Embedding (KANICE) by applying it to diverse datasets. Specifically, the research focuses on testing KANICE on plant classification and object classification tasks. By addressing the future directions outlined in the original research paper, this study aims to validate KANICE's performance on larger datasets and more complex tasks, providing insights into its practical applicability and potential improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithms**\n",
    "we will implement the KANICE architecture, which integrates Kolmogorov-Arnold Networks (KAN) with Interactive Convolutional Blocks (ICBs) to classify images of  different plant species and distinguish between different objects.\n",
    "\n",
    "\n",
    "1-KANICE Architecture:\n",
    "\n",
    "- Interactive Convolutional Blocks (ICBs): These blocks will be used for initial feature extraction from images. Each ICB will consist of parallel convolutional paths with different kernel sizes (3x3 and 5x5), followed by an interaction step that combines the outputs using element-wise multiplication.\n",
    "- KANLinear Layers: These layers will replace traditional fully connected layers, leveraging the Kolmogorov-Arnold representation theorem for enhanced function approximation. This will help model complex relationships in the data effectively.\n",
    "\n",
    "\n",
    "2-Dataset\n",
    "\n",
    "- Plant Classification: A dataset comprising images of various plant species will be collected, ensuring a balanced representation across all species to facilitate accurate classification.\n",
    "\n",
    "- Object Classification: A publicly available dataset, such as torchvision.datasets.CIFAR10, will be utilized to cover a wide range of objects, ensuring diverse and comprehensive object classification.\n",
    "\n",
    "\n",
    "3-Training and Evaluation:\n",
    "\n",
    "- The models will be trained using standard optimization techniques, with appropriate learning rates and batch sizes.\n",
    "- Performance metrics will include accuracy, precision, recall, and F1 score to evaluate the model's effectiveness on both classification tasks.\n",
    "\n",
    "\n",
    "**Expected Milestones**\n",
    "\n",
    "| Milestone  1 | Milestone 2| Milestone 3 |\n",
    "|----------|----------|----------|\n",
    "| Dataset Collection and Preprocessing - January 10, 2024   |  Model Implementation and Initial Training - January 20, 2024   | Model Evaluation and Final Report Preparation - January 26, 2024  |\n",
    "\n",
    "\n",
    "**Team Roles**\n",
    "\n",
    "- Areej Bawazir: Data Collection and Preprocessing\n",
    "Responsible for gathering images of the two plant species and the object classification dataset. This includes sourcing the data, cleaning it, and performing any necessary preprocessing, such as resizing and normalization.\n",
    "\n",
    "- Reem Qaid: Model Implementation and Training\n",
    "Tasked with implementing the KANICE architecture in a suitable deep learning framework (e.g., TensorFlow or PyTorch). This includes coding the Interactive Convolutional Blocks and KANLinear layers, setting up the training loop, and fine-tuning hyperparameters during the training process.\n",
    "\n",
    "- Ghaida Aluhaiby: Evaluation and Report Writing\n",
    "Focused on evaluating the model's performance using defined metrics, documenting results, and preparing the final report. This member will ensure that all sections of the report are cohesive and accurately represent the team's work, including writing the introduction, method, results, and conclusion.\n",
    "\n",
    "\n",
    "**References**\n",
    "\n",
    "Goodfellow, I., Bengio, Y., & Courville, A. Deep Learning. MIT Press.\n",
    "Ferdaus, M. M., Abdelguerfi, M., Ioup, E., et al. (2024). KANICE: Kolmogorov-Arnold Networks with Interactive Convolutional Elements. Proceedings of The 4th International Conference on AI ML Systems. Available at: arXiv and GitHub.https://arxiv.org/pdf/2410.17172v1. and https://github.com/m-ferdaus/kanice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(nbfile) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMore than one ipynb file. Using the first one.  nbfile=\u001b[39m\u001b[38;5;124m'\u001b[39m, nbfile)\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m io\u001b[38;5;241m.\u001b[39mopen(nbfile[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      8\u001b[0m     nb \u001b[38;5;241m=\u001b[39m nbformat\u001b[38;5;241m.\u001b[39mread(f, nbformat\u001b[38;5;241m.\u001b[39mNO_CONVERT)\n\u001b[0;32m      9\u001b[0m word_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import nbformat\n",
    "import glob\n",
    "nbfile = glob.glob('Project Proposal and Report Example.ipynb')\n",
    "if len(nbfile) > 1:\n",
    "    print('More than one ipynb file. Using the first one.  nbfile=', nbfile)\n",
    "with io.open(nbfile[0], 'r', encoding='utf-8') as f:\n",
    "    nb = nbformat.read(f, nbformat.NO_CONVERT)\n",
    "word_count = 0\n",
    "for cell in nb.cells:\n",
    "    if cell.cell_type == \"markdown\":\n",
    "        word_count += len(cell['source'].replace('#', '').lstrip().split(' '))\n",
    "print('Word count for file', nbfile[0], 'is', word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
